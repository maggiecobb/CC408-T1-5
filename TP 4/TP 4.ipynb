{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ciencia de datos - Primavera 2024\n",
    "##### Integrantes: Magdalena Cobb, Pedro García Vassallo, Marcos Olavarría\n",
    "### Trabajo Práctico 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LECTURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes y librerías a usar\n",
    "# CHEQUEO DE CAMBIOS\n",
    "#!pip install openpyxl\n",
    "#!pip install statsmodels\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura del documento\n",
    "indi_2024 = pd.read_excel('C:/Users/SAMSUNG/Documents/GitHub/CC408-T1-5/TP 4/usu_individual_T124.xlsx') \n",
    "indi_2004 = pd.read_stata('C:/Users/SAMSUNG/Documents/GitHub/CC408-T1-5/TP 4/Individual_t104.dta')\n",
    "hoga_2024 = pd.read_excel('C:/Users/SAMSUNG/Documents/GitHub/CC408-T1-5/TP 4/usu_hogar_T124.xlsx')\n",
    "hoga_2004 = pd.read_stata('C:/Users/SAMSUNG/Documents/GitHub/CC408-T1-5/TP 4/Individual_t104.dta')\n",
    "\n",
    "# Maggie: C:/Users/magda/OneDrive/Documents/GitHub/CC408-T1-5/TP 4/\n",
    "# Peter: 'C:/Users/SAMSUNG/Documents/GitHub/CC408-T1-5/TP4/Individual_t104.dta\n",
    "# Marck: 'C:/Users/marcos.olavarria/Documents/GitHub/CC408/TP 4/usu_hogar_T124.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Selección de GBA y merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Eliminen todas las observaciones que no corresponden a los\n",
    "aglomerados de Ciudad Autónoma de Buenos Aires o Gran Buenos\n",
    "Aires, y unan ambos trimestres en una sola base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtramos las bases para tener solo Buenos Aires\n",
    "ind_2024 = indi_2024[(indi_2024['AGLOMERADO'] == 33) | (indi_2024['AGLOMERADO'] == 32)]\n",
    "hog_2024 = hoga_2024[(hoga_2024['AGLOMERADO'] == 33) | (hoga_2024 ['AGLOMERADO'] == 32 )]\n",
    "\n",
    "# \n",
    "ind_2004 = indi_2004[(indi_2004['aglomerado'] == 'Ciudad de Buenos Aires') | (indi_2004 ['aglomerado'] == 'Partidos del GBA' )]\n",
    "hog_2004 = hoga_2004[(hoga_2004['aglomerado'] == 'Ciudad de Buenos Aires') | (hoga_2004 ['aglomerado'] == 'Partidos del GBA' )]\n",
    "\n",
    "ind_2004.columns = ind_2004.columns.str.upper()\n",
    "hog_2004.columns = ind_2004.columns.str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_con = ['ANO4', 'TRIMESTRE', 'MAS_500', 'REGION', 'AGLOMERADO', 'PONDERA',\n",
    "       'ITF', 'DECIFR', 'IDECIFR', 'RDECIFR', 'GDECIFR', 'PDECIFR', 'ADECIFR',\n",
    "       'IPCF', 'DECCFR', 'IDECCFR', 'RDECCFR', 'GDECCFR', 'PDECCFR', 'ADECCFR',\n",
    "       'PONDIH'] \n",
    "\n",
    "lista_con2 = [\n",
    "    'COMPONENTE', 'H15', 'ANO4', 'TRIMESTRE', 'REGION', 'MAS_500', 'AGLOMERADO',\n",
    "    'PONDERA', 'CH03', 'CH04', 'CH06', 'CH07', 'CH08', 'CH09', 'CH10', 'CH11', 'CH12',\n",
    "    'CH13', 'CH14', 'CH15', 'CH15_COD', 'CH16', 'CH16_COD', 'NIVEL_ED', 'ESTADO',\n",
    "    'CAT_OCUP', 'CAT_INAC', 'PP02C1', 'PP02C2', 'PP02C3', 'PP02C4', 'PP02C5', 'PP02C6',\n",
    "    'PP02C7', 'PP02C8', 'PP02E', 'PP02H', 'PP02I', 'PP03C', 'PP03D', 'PP3E_TOT', 'PP3F_TOT',\n",
    "    'PP03G', 'PP03H', 'PP03I', 'PP03J', 'INTENSI', 'PP04A', 'PP04B_COD', 'PP04B1', 'PP04B2',\n",
    "    'PP04B3_MES', 'PP04B3_ANO', 'PP04B3_DIA', 'PP04C', 'PP04C99', 'PP04D_COD', 'PP04G',\n",
    "    'PP05B2_MES', 'PP05B2_ANO', 'PP05B2_DIA', 'PP05C_1', 'PP05C_2', 'PP05C_3', 'PP05E',\n",
    "    'PP05F', 'PP05H', 'PP06A', 'PP06C', 'PP06D', 'PP06E', 'PP06H', 'PP07A', 'PP07C', 'PP07D',\n",
    "    'PP07E', 'PP07F1', 'PP07F2', 'PP07F3', 'PP07F4', 'PP07F5', 'PP07G1', 'PP07G2', 'PP07G3',\n",
    "    'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K', 'PP08D1', 'PP08D4', 'PP08F1',\n",
    "    'PP08F2', 'PP08J1', 'PP08J2', 'PP08J3', 'PP09A', 'PP09A_ESP', 'PP09B', 'PP09C', 'PP09C_ESP',\n",
    "    'PP10A', 'PP10C', 'PP10D', 'PP10E', 'PP11A', 'PP11B_COD', 'PP11B1', 'PP11B2_MES',\n",
    "    'PP11B2_ANO', 'PP11B2_DIA', 'PP11C', 'PP11C99', 'PP11D_COD', 'PP11G_ANO', 'PP11G_MES',\n",
    "    'PP11G_DIA', 'PP11L', 'PP11L1', 'PP11M', 'PP11N', 'PP11O', 'PP11P', 'PP11Q', 'PP11R', 'PP11S',\n",
    "    'PP11T', 'P21', 'DECOCUR', 'IDECOCUR', 'RDECOCUR', 'GDECOCUR', 'PDECOCUR', 'ADECOCUR', 'TOT_P12',\n",
    "    'P47T', 'DECINDR', 'IDECINDR', 'RDECINDR', 'GDECINDR', 'PDECINDR', 'ADECINDR', 'V2_M', 'V3_M',\n",
    "    'V4_M', 'V5_M', 'V8_M', 'V9_M', 'V10_M', 'V11_M', 'V12_M', 'V18_M', 'V19_AM', 'V21_M', 'T_VI', \n",
    "    'ITF', 'DECIFR', 'IDECIFR', 'RDECIFR', 'GDECIFR', 'PDECIFR', 'ADECIFR', 'IPCF',\n",
    "    'DECCFR', 'IDECCFR', 'RDECCFR', 'GDECCFR', 'PDECCFR', 'ADECCFR', 'PJ1_1', 'PJ2_1',\n",
    "    'PJ3_1', 'IDIMPP'\n",
    "]\n",
    "\n",
    "ind_2024 = ind_2024.drop(columns = (lista_con))\n",
    "ind_2004 = ind_2004.drop (columns = (lista_con2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_04 = pd.merge(ind_2004, hog_2004, on=['CODUSU', 'NRO_HOGAR'], how='outer')\n",
    "combined_24 = pd.merge(ind_2024,hog_2024,on=[\"CODUSU\",\"NRO_HOGAR\"], how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Limpieza\n",
    "Limpien la base de datos tomando criterios que hagan sentido. Explicar\n",
    "cualquier decisión como el tratamiento de valores faltantes (missing\n",
    "values), extremos (outliers), o variables categóricas. Justifique sus\n",
    "decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se usa base de datos:\n",
    "# 2024: \n",
    "# - combined_24\n",
    "\n",
    "# 2004:\n",
    "# - combined_04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               P47T         CH14        PP03C        PP03D     PP3E_TOT  \\\n",
      "count  5.718000e+03  5718.000000  5718.000000  5718.000000  5718.000000   \n",
      "mean   2.311709e+05     4.026058     0.994404     0.083945    34.736097   \n",
      "std    6.155965e+05    13.434616     0.279252     0.458093    17.566290   \n",
      "min    0.000000e+00     0.000000     0.000000     0.000000     0.000000   \n",
      "25%    0.000000e+00     2.000000     1.000000     0.000000    36.000000   \n",
      "50%    1.000000e+05     2.000000     1.000000     0.000000    36.000000   \n",
      "75%    3.000000e+05     2.000000     1.000000     0.000000    36.000000   \n",
      "max    3.260000e+07    99.000000     2.000000     9.000000   999.000000   \n",
      "\n",
      "          PP3F_TOT        PP03G        PP03H        PP03I        PP03J  ...  \\\n",
      "count  5718.000000  5718.000000  5718.000000  5718.000000  5718.000000  ...   \n",
      "mean      1.194299     1.928821     0.089192     1.945261     1.935992  ...   \n",
      "std      26.587341     0.257146     0.438347     0.284848     0.264040  ...   \n",
      "min       0.000000     1.000000     0.000000     1.000000     1.000000  ...   \n",
      "25%       0.000000     2.000000     0.000000     2.000000     2.000000  ...   \n",
      "50%       0.000000     2.000000     0.000000     2.000000     2.000000  ...   \n",
      "75%       0.000000     2.000000     0.000000     2.000000     2.000000  ...   \n",
      "max     999.000000     2.000000     9.000000     9.000000     9.000000  ...   \n",
      "\n",
      "             PP08D4        PP08F1         PP08F2        PP08J1        PP08J2  \\\n",
      "count   5718.000000  5.718000e+03    5718.000000  5.718000e+03  5.718000e+03   \n",
      "mean       5.246590  1.875114e+03     171.213711  1.457826e+04  1.211263e+03   \n",
      "std      280.508609  3.005187e+04    4428.969894  7.139622e+04  5.083769e+04   \n",
      "min        0.000000  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
      "25%        0.000000  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
      "50%        0.000000  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
      "75%        0.000000  0.000000e+00       0.000000  0.000000e+00  0.000000e+00   \n",
      "max    20000.000000  1.400000e+06  280000.000000  1.997500e+06  3.600000e+06   \n",
      "\n",
      "             PP08J3        PP09A        PP09B   PP09C     CH15_COD  \n",
      "count   5718.000000  5718.000000  5718.000000  5718.0  5718.000000  \n",
      "mean      20.461700     1.902938     0.000175     0.0    91.181707  \n",
      "std      897.648764     0.410022     0.013224     0.0    43.003484  \n",
      "min        0.000000     0.000000     0.000000     0.0    10.000000  \n",
      "25%        0.000000     2.000000     0.000000     0.0    86.000000  \n",
      "50%        0.000000     2.000000     0.000000     0.0    86.000000  \n",
      "75%        0.000000     2.000000     0.000000     0.0    86.000000  \n",
      "max    50000.000000     9.000000     1.000000     0.0   999.000000  \n",
      "\n",
      "[8 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de datos tomando criterios que hagan sentido\n",
    "combined_24\n",
    "print(combined_24.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de observaciones con valores negativos: 1333\n"
     ]
    }
   ],
   "source": [
    "# Identificar filas con valores negativos\n",
    "negativos = combined_24.lt(0).any(axis=1)\n",
    "\n",
    "# Contar las observaciones con valores negativos\n",
    "conteo_negativos = negativos.sum()\n",
    "print(f\"Número de observaciones con valores negativos: {conteo_negativos}\")\n",
    "\n",
    "combined_24 = combined_24 [~negativos]\n",
    "\n",
    "\n",
    "# Le tengo que sacar los valores altos a las sig columnas:\n",
    "# CH14\n",
    "# PP3E_TOT\n",
    "# PP3F_TOT\n",
    "# PP08D4\n",
    "# PP08F2\n",
    "# PP08J3\n",
    "# CH15_COD\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores Faltantes\n",
    "- 2004 no tiene\n",
    "- 2024 se eliminaron los más brutos y se imputaron por la mediana los menos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2004\n",
    "no tiene columnas con valores Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información general:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32911 entries, 0 to 32910\n",
      "Columns: 350 entries, CODUSU to IDIMPP_y\n",
      "dtypes: category(188), float64(91), object(71)\n",
      "memory usage: 46.6+ MB\n",
      "None\n",
      "Columnas con valores faltantes: []\n"
     ]
    }
   ],
   "source": [
    "# Valores faltantes. Reviso si hay valores Nan\n",
    "print(\"Información general:\")\n",
    "print(combined_04.info())  # Muestra tipos de datos y conteos (puedes identificar NaN indirectamente)\n",
    "\n",
    "columnas_con_nan = combined_04.columns[(combined_04.isnull().sum() / len(combined_04)) > 0].tolist()\n",
    "\n",
    "# Mostrar las columnas\n",
    "print(\"Columnas con valores faltantes:\", columnas_con_nan)\n",
    "\n",
    "# No Hay valores Nan, no hay más analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2024\n",
    "- Tiene columnas con valores Nan\n",
    "- Se revisa la proporción de Nan que hay por columna\n",
    "- Elimino las columnas que tienen proporción de Nans > 90\n",
    "- Imputo por la mediana en las columnas que su proporción de Nans < 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información general:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7051 entries, 0 to 7050\n",
      "Columns: 242 entries, CODUSU to VII2_4\n",
      "dtypes: float64(106), int64(126), object(10)\n",
      "memory usage: 13.0+ MB\n",
      "None\n",
      "Columnas con valores faltantes: ['CH14', 'CH15_COD', 'CH16_COD', 'IMPUTA', 'PP03C', 'PP03D', 'PP3E_TOT', 'PP3F_TOT', 'PP03G', 'PP03H', 'PP03I', 'PP03J', 'INTENSI', 'PP04A', 'PP04B_COD', 'PP04B1', 'PP04B2', 'PP04B3_MES', 'PP04B3_ANO', 'PP04B3_DIA', 'PP04C', 'PP04C99', 'PP04D_COD', 'PP04G', 'PP05B2_MES', 'PP05B2_ANO', 'PP05B2_DIA', 'PP05C_1', 'PP05C_2', 'PP05C_3', 'PP05E', 'PP05F', 'PP05H', 'PP06A', 'PP06C', 'PP06D', 'PP06E', 'PP06H', 'PP07A', 'PP07C', 'PP07D', 'PP07E', 'PP07F1', 'PP07F2', 'PP07F3', 'PP07F4', 'PP07F5', 'PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K', 'PP08D1', 'PP08D4', 'PP08F1', 'PP08F2', 'PP08J1', 'PP08J2', 'PP08J3', 'PP09A', 'PP09A_ESP', 'PP09B', 'PP09C', 'PP09C_ESP', 'PP10A', 'PP10C', 'PP10D', 'PP10E', 'PP11A', 'PP11B_COD', 'PP11B1', 'PP11B2_MES', 'PP11B2_ANO', 'PP11B2_DIA', 'PP11C', 'PP11C99', 'PP11D_COD', 'PP11G_ANO', 'PP11G_MES', 'PP11G_DIA', 'PP11L', 'PP11L1', 'PP11M', 'PP11N', 'PP11O', 'PP11P', 'PP11Q', 'PP11R', 'PP11S', 'PP11T', 'IDECOCUR', 'PDECOCUR', 'P47T', 'IDECINDR', 'PDECINDR', 'IV1_ESP', 'IV3_ESP', 'IV7_ESP', 'II7_ESP', 'II8_ESP', 'IDECIFR', 'PDECIFR', 'IDECCFR', 'PDECCFR']\n"
     ]
    }
   ],
   "source": [
    "# Valores faltantes. Reviso si hay valores Nan\n",
    "print(\"Información general:\")\n",
    "print(combined_24.info())  # Muestra tipos de datos y conteos (puedes identificar NaN indirectamente)\n",
    "\n",
    "columnas_con_nan = combined_24.columns[(combined_24.isnull().sum() / len(combined_24)) > 0].tolist()\n",
    "\n",
    "# Mostrar las columnas\n",
    "print(\"Columnas con valores faltantes:\", columnas_con_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengo las columnas que contienen datos faltantes, reviso que proporción de los valores faltas\n",
    "\n",
    "# Porcentaje de valores faltantes por columna\n",
    "lista_nans_poco = []\n",
    "lista_nans_50 = []\n",
    "lista_nans_60 = []\n",
    "lista_nans_70 = []\n",
    "lista_nans_80 = []\n",
    "lista_nans_90 = []\n",
    "lista_nans_100 = []\n",
    "\n",
    "for i in columnas_con_nan:\n",
    "    proporcion_columna = combined_24[i].isnull().sum() / len(combined_24) * 100\n",
    "    \n",
    "    if proporcion_columna < 50:\n",
    "        lista_nans_poco.append(i)\n",
    "\n",
    "    elif proporcion_columna >= 50 and proporcion_columna < 60:\n",
    "        lista_nans_50.append(i)\n",
    "\n",
    "    elif proporcion_columna >= 60 and proporcion_columna < 70:\n",
    "        lista_nans_60.append(i)\n",
    "\n",
    "    elif proporcion_columna >= 70 and proporcion_columna < 80:\n",
    "        lista_nans_70.append(i)\n",
    "\n",
    "    elif proporcion_columna >= 80 and proporcion_columna < 90:\n",
    "        lista_nans_80.append(i)\n",
    "\n",
    "    elif proporcion_columna >= 90 and proporcion_columna < 100:\n",
    "        lista_nans_90.append(i)\n",
    "\n",
    "    elif proporcion_columna == 100:\n",
    "        lista_nans_100.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino las variables que tienen más de 90% de datos Nan. Me fijo antes cuantas columnas son\n",
    "columnas_elim = lista_nans_90 + lista_nans_100\n",
    "combined_24 = combined_24.drop(columns = (columnas_elim))\n",
    "\n",
    "# Imputo por la mediana en las columnas que su proporción de Nans < 90\n",
    "columnas_ok = lista_nans_poco + lista_nans_50 +lista_nans_60 + lista_nans_70 + lista_nans_80\n",
    "combined_24 = combined_24[columnas_ok].fillna(combined_24[columnas_ok].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "# COMBINED 04\n",
    "import pandas as pd\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado combined_04\n",
    "# Primero seleccionamos solo las columnas numéricas para el análisis\n",
    "numerical_columns = combined_04.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculamos los cuartiles y el IQR solo para las columnas numéricas\n",
    "Q1 = combined_04[numerical_columns].quantile(0.25)\n",
    "Q3 = combined_04[numerical_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Definir los límites inferior y superior\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Detectar outliers (donde los valores están fuera de los límites)\n",
    "outliers = (combined_04[numerical_columns] < lower_limit) | (combined_04[numerical_columns] > upper_limit)\n",
    "\n",
    "# Mostrar una vista de los outliers por fila\n",
    "outliers_data = combined_04[outliers.any(axis=1)]\n",
    "\n",
    "# Opcional: Mostrar el total de outliers por columna\n",
    "outliers_count = outliers.sum()\n",
    "print(f\"Total de outliers por columna: \\n{outliers_count}\")\n",
    "\n",
    "# Mostrar las filas que contienen outliers\n",
    "print(f\"Filas con outliers: \\n{outliers_data}\")\n",
    "\n",
    "# Eliminar filas con outliers\n",
    "#combined_04_no_outliers = combined_04[~outliers.any(axis=1)]\n",
    "#print(f\"Datos después de eliminar outliers: \\n{combined_04_no_outliers.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINED 24\n",
    "import pandas as pd\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado combined_04\n",
    "# Primero seleccionamos solo las columnas numéricas para el análisis\n",
    "numerical_columns = combined_24.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculamos los cuartiles y el IQR solo para las columnas numéricas\n",
    "Q1 = combined_24[numerical_columns].quantile(0.25)\n",
    "Q3 = combined_24[numerical_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Definir los límites inferior y superior\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "# Detectar outliers (donde los valores están fuera de los límites)\n",
    "outliers = (combined_24[numerical_columns] < lower_limit) | (combined_24[numerical_columns] > upper_limit)\n",
    "\n",
    "# Mostrar una vista de los outliers por fila\n",
    "outliers_data = combined_24[outliers.any(axis=1)]\n",
    "\n",
    "# Opcional: Mostrar el total de outliers por columna\n",
    "outliers_count = outliers.sum()\n",
    "print(f\"Total de outliers por columna: \\n{outliers_count}\")\n",
    "\n",
    "# Mostrar las filas que contienen outliers\n",
    "print(f\"Filas con outliers: \\n{outliers_data}\")\n",
    "\n",
    "# Eliminar filas con outliers\n",
    "#combined_04_no_outliers = combined_04[~outliers.any(axis=1)]\n",
    "#print(f\"Datos después de eliminar outliers: \\n{combined_04_no_outliers.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categoricas\n",
    "\n",
    "lista = combined_04.columns.tolist()\n",
    "lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categoricas 04\n",
    "origin_dummies = pd.get_dummies(auto['origin'], prefix='origin')\n",
    "# Concatenamos con el df original\n",
    "auto_d = pd.concat([auto, origin_dummies], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CH05 (fecha de nacimiento)\n",
    "# CH06 \n",
    "# PP03D\n",
    "# PP3E_TOT\n",
    "# PP3F_TOT\n",
    "# PP04B2\n",
    "# PP04B3_MES\n",
    "# PP04B3_ANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Identificar las columnas numéricas continuas\n",
    "numerical_columns = combined_04.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(\"Columnas numéricas continuas:\")\n",
    "print(numerical_columns)\n",
    "\n",
    "# Paso 2: Identificar las columnas categóricas\n",
    "categorical_columns = combined_04.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Columnas categóricas:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# Paso 3: Convertir las columnas categóricas en variables dummies\n",
    "combined_04_dummies = pd.get_dummies(combined_04, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(\"DataFrame con variables dummy:\")\n",
    "print(combined_04_dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir una columna numérica discreta en categórica (si tiene pocos valores únicos)\n",
    "combined_04['H15_x'] = combined_04['H15_x'].astype('category')\n",
    "\n",
    "# Identificar las columnas categóricas después de la conversión\n",
    "categorical_columns = combined_04.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Columnas categóricas después de la conversión:\")\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericas = [\"CH05\",\"CH06\",\"PP03D\",\"PP3E_TOT\",\"PP3F_TOT\",\"PP04B2\",\"PP04B3_MES\",\"PP04B3_ANO\",\"PP04B3_DIA\",\"CODUSU\",\"NRO_HOGAR\",\"IV2\",\"IV1_ESP\",\"IV7_ESP\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for item in numericas:\n",
    "    if item in lista:\n",
    "        lista.remove(item)\n",
    "\n",
    "\n",
    "\n",
    "# CH05 (fecha de nacimiento)\n",
    "# CH06 \n",
    "# PP03D\n",
    "# PP3E_TOT\n",
    "# PP3F_TOT\n",
    "# PP04B2\n",
    "# PP04B3_MES\n",
    "# PP04B3_ANO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificacion y regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se separa X e y a partir de la selección de columnas numericas\n",
    "#ESTO HAY QUE UPDATEARLO CON LAS BASES FINALES\n",
    "X_2004 = again_04_selec.drop(columns=['desocupado'])\n",
    "y_2004 = again_04_selec['desocupado']\n",
    "X_2024 = again_24_selec.drop(columns=['desocupado'])\n",
    "y_2024 = again_24_selec['desocupado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partir la base train test para cada ano. \n",
    "#Semilla 101\n",
    "#agregar columna de unos (1)\n",
    "x_train_24, x_test_24, y_train_24, y_test_24 = train_test_split(X_2024, y_2024, test_size=0.3, random_state=101)\n",
    "x_train_04, x_test_04, y_train_04, y_test_04 = train_test_split(X_2004, y_2004, test_size=0.3, random_state=101)\n",
    "\n",
    "x_train_04 = sm.add_constant(x_train_04) #AGREGO COLUMNA DE UNOS\n",
    "x_train_24 = sm.add_constant(x_train_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANTE ESTANDARIZAR ANTES DE USAR METODOS DE REGULARIZACION\n",
    "# Iniciamos el Standard Scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Estandarizamos las observaciones de entrenamiento\n",
    "x_train_04_transformed = pd.DataFrame(sc.fit_transform(x_train_04), index=x_train_04.index, columns=x_train_04.columns)\n",
    "x_train_24_transformed = pd.DataFrame(sc.fit_transform(x_train_24), index=x_train_24.index, columns=x_train_24.columns)\n",
    "\n",
    "# Estandarizamos las observaciones de test\n",
    "x_test_04_transformed = pd.DataFrame(sc.transform(x_test_04), index=x_test_04.index, columns=x_test_04.columns)\n",
    "x_test_24_transformed = pd.DataFrame(sc.transform(x_test_24), index=x_test_24.index, columns=x_test_24.columns)  \n",
    "\n",
    "# Estadisticas luego de estandarizar\n",
    "x_train_04_transformed.describe().T\n",
    "x_train_24_transformed.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_04' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Regresion logistica con penalidad LASSO (todos estos pasos se hacen para cada ano)\u001b[39;00m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#### Regresion logistica para 2004\u001b[39;00m\n",
      "\u001b[1;32m----> 3\u001b[0m log04_lasso \u001b[38;5;241m=\u001b[39m LogisticRegression(penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m, C\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m), solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(x_train_04, y_train_04)\n",
      "\u001b[0;32m      4\u001b[0m y_test_pred_score_04_lasso \u001b[38;5;241m=\u001b[39m log04_lasso\u001b[38;5;241m.\u001b[39mpredict_proba(x_train_04)[:,\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;32m      5\u001b[0m y_pred_log04_lasso \u001b[38;5;241m=\u001b[39m log04_lasso\u001b[38;5;241m.\u001b[39mpredict(x_test_04)\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train_04' is not defined"
     ]
    }
   ],
   "source": [
    "#Regresion logistica con penalidad LASSO (todos estos pasos se hacen para cada ano)\n",
    "#### Regresion logistica para 2004\n",
    "alpha = 1\n",
    "log04_lasso = LogisticRegression(penalty='l1', C=(1/alpha), solver='liblinear').fit(x_train_04, y_train_04)\n",
    "y_test_pred_score_04_lasso = log04_lasso.predict_proba(x_train_04)[:,1]\n",
    "y_pred_log04_lasso = log04_lasso.predict(x_test_04)\n",
    "\n",
    "#### Regresion logistica para 2024\n",
    "log24_lasso = LogisticRegression(penalty='l1', C=(1/alpha), solver='liblinear').fit(x_train_24, y_train_24)\n",
    "y_test_pred_score_24_lasso = log24_lasso.predict_proba(x_train_24)[:,1]\n",
    "y_pred_log24_lasso = log24_lasso.predict(x_test_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Medidas de precision Regresion logistica LASSO para 2004\n",
    "#Matriz de confusion\n",
    "conflog04_lasso = confusion_matrix(y_test_04, y_pred_log04_lasso) \n",
    "print(\"Matriz de confusion (Reg Log LASSO 2004)\\n\", conflog04_lasso)\n",
    "\n",
    "#AUC\n",
    "auc_log04_lasso = roc_auc_score(y_test_04, y_pred_log04_lasso)\n",
    "print('\\nAUC Reg Log (2004): %.4f' %auc_log04_lasso)\n",
    "\n",
    "#Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_04, y_pred_log04_lasso)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_log04_lasso, estimator_name='Reg Log')\n",
    "display.plot()  \n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.show() \n",
    "\n",
    "#Accuracy\n",
    "accuracy_log04_lasso = accuracy_score(y_test_04, y_pred_log04_lasso)\n",
    "print(\"La accuracy del modelo (Reg Log LASSO 2004) es: %.4f\" %accuracy_log04_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Medidas de precision Regresion logistica LASSO para 2024\n",
    "#Matriz de confusion\n",
    "conflog24_lasso = confusion_matrix(y_test_24, y_pred_log24_lasso) \n",
    "print(\"Matriz de confusion (Reg Log LASSO 2024)\\n\", conflog24_lasso)\n",
    "\n",
    "#AUC\n",
    "auc_log24_lasso = roc_auc_score(y_test_24, y_pred_log24_lasso)\n",
    "print('\\nAUC Reg Log LASSO (2024): %.4f' %auc_log24_lasso)\n",
    "\n",
    "#Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_24, y_pred_log24_lasso)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_log24_lasso, estimator_name='Reg Log')\n",
    "display.plot()  \n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.show() \n",
    "\n",
    "#Accuracy\n",
    "accuracy_log24_lasso = accuracy_score(y_test_24, y_pred_log24_lasso)\n",
    "print(\"La accuracy del modelo (Reg Log LASSO 2024) es: %.4f\" %accuracy_log24_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regresion logistica con penalidad ridge (todos estos pasos se hacen para cada ano)\n",
    "#### Regresion logistica para 2004\n",
    "alpha = 1\n",
    "log04_ridge = LogisticRegression(penalty='l2', C=(1/alpha), solver='liblinear').fit(x_train_04, y_train_04)\n",
    "y_test_pred_score_04_ridge = log04_ridge.predict_proba(x_train_04)[:,1]\n",
    "y_pred_log04_ridge = log04_ridge.predict(x_test_04)\n",
    "\n",
    "#### Regresion logistica para 2024\n",
    "log24_ridge = LogisticRegression(penalty='l2', C=(1/alpha), solver='liblinear').fit(x_train_24, y_train_24)\n",
    "y_test_pred_score_24_ridge = log24_ridge.predict_proba(x_train_24)[:,1]\n",
    "y_pred_log24_ridge = log24_ridge.predict(x_test_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas04 = ['','']\n",
    "for col in categoricas04:\n",
    "    dummies = pd.get_dummies(combined_04[col], prefix=col)\n",
    "    combined_04 = pd.concat([combined_04, dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Medidas de precision Regresion logistica ridge para 2004\n",
    "#Matriz de confusion\n",
    "conflog04_ridge = confusion_matrix(y_test_04, y_pred_log04_ridge) \n",
    "print(\"Matriz de confusion (Reg Log ridge 2004)\\n\", conflog04_ridge)\n",
    "\n",
    "#AUC\n",
    "auc_log04_ridge = roc_auc_score(y_test_04, y_pred_log04_ridge)\n",
    "print('\\nAUC Reg Log ridge (2004): %.4f' %auc_log04_ridge)\n",
    "\n",
    "#Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_04, y_pred_log04_ridge)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_log04_ridge, estimator_name='Reg Log')\n",
    "display.plot()  \n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.show() \n",
    "\n",
    "#MSE\n",
    "ecm_a1_r04 = mean_squared_error(y_test_04, y_pred_log04_ridge)\n",
    "\n",
    "#Accuracy\n",
    "accuracy_log04_ridge = accuracy_score(y_test_04, y_pred_log04_ridge)\n",
    "print(\"La accuracy del modelo (Reg Log ridge 2004) es: %.4f\" %accuracy_log04_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Medidas de precision Regresion logistica ridge para 2024\n",
    "#Matriz de confusion\n",
    "conflog24_ridge = confusion_matrix(y_test_24, y_pred_log24_ridge) \n",
    "print(\"Matriz de confusion (Reg Log ridge 2024)\\n\", conflog04_ridge)\n",
    "\n",
    "#AUC\n",
    "auc_log24_ridge = roc_auc_score(y_test_24, y_pred_log24_ridge)\n",
    "print('\\nAUC Reg Log ridge (2024): %.4f' %auc_log24_ridge)\n",
    "\n",
    "#Curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test_24, y_pred_log24_ridge)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc_log24_ridge, estimator_name='Reg Log')\n",
    "display.plot()  \n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.show() \n",
    "\n",
    "#Accuracy\n",
    "accuracy_log24_ridge = accuracy_score(y_test_24, y_pred_log24_ridge)\n",
    "print(\"La accuracy del modelo (Reg Log ridge 2024) es: %.4f\" %accuracy_log24_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation para elegir el alpha para los metodos de regularizacion\n",
    "#boxplot usando seaborn: distribucion del error de prediccion para cada alpha\n",
    "\n",
    "#lineplot para lasso: promedio de la proporcion de variables ignoradas por el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparar resultados lasso y ridge 2004 vs 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
